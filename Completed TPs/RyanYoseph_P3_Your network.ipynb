{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"P3 Your network.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HGboTXoGKNNz","colab_type":"text"},"source":["- Build your own network for Plant Seedlings Classification (once you understand the assignement of dog vs. cat classification, you should do this part not more than 8 hours).\n","\n","https://www.kaggle.com/c/plant-seedlings-classification/data\n","\n","- You have to resend your code and a report to the teacher (via Moodle) before 11th april.\n","\n","- Your network maynot work perfectly, do not worry about that. We rather need you to know how to construct an AI system (we will check your comprehension orally).\n","\n","- Your should follow the similar steps as in assignement of dog vs. cat classification. This means that, you should build a network with several Convolutional, Dense layers, you should use transfer learning (with VGG, Inception,...) as well as data augmentation...\n","\n","- If you try different architectures, you should detail that in the report (different architectures with different performance). \n","\n","- You can inspire by the code available but not copy-paste (the codes for loading/splitting the dataset are available, but you need to understand...)\n"]},{"cell_type":"code","metadata":{"id":"qlbU7ZXw6gaf","colab_type":"code","colab":{}},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","import keras\n","keras.__version__\n","from keras import layers\n","from keras import models\n","from keras.layers import Dense, Activation\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras import optimizers"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhty57zd6hBX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":123},"outputId":"85b7fcc6-9969-4932-d7d3-179d5770583b","executionInfo":{"status":"ok","timestamp":1590615106317,"user_tz":300,"elapsed":24507,"user":{"displayName":"Emma K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTDcVUh3zyIi0z9xgvjNPASwgXaX71FH-FLJJSLA=s64","userId":"15689506037919733965"}}},"source":["import shutil\n","\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vxUZnGrw6mJz","colab_type":"code","colab":{}},"source":["# The directory where we will store our smaller dataset\n","base_dir = '/content/drive/My Drive/AIandBigData/TP4P3/plants'\n","\n","# Directories for our training, validation and test splits\n","train_dir = os.path.join(base_dir, 'train')\n","validation_dir = os.path.join(base_dir, 'validation')\n","test_dir = os.path.join(base_dir, 'test')\n","\n","#Train Directory \n","# Directory with our training blackgrass pictures\n","train_blackgrass_dir = os.path.join(train_dir, 'Black-grass')\n","# Directory with our training charlock pictures\n","train_charlock_dir = os.path.join(train_dir, 'Charlock')\n","# Directory with our training cleavers pictures\n","train_cleavers_dir = os.path.join(train_dir, 'Cleavers')\n","# Directory with our training common chickweed pictures\n","train_common_chickweed_dir = os.path.join(train_dir, 'Common Chickweed')\n","# Directory with our training Common Wheat pictures\n","train_common_wheat_dir = os.path.join(train_dir, 'Common wheat')\n","# Directory with our training Fat Hen pictures\n","train_fat_hen_dir = os.path.join(train_dir, 'Fat Hen')\n","# Directory with our training Loose Silky-bent pictures\n","train_loose_silkybent_dir = os.path.join(train_dir, 'Loose Silky-bent')\n","# Directory with our training maize pictures\n","train_maize_dir = os.path.join(train_dir, 'Maize')\n","# Directory with our training scentless mayweed pictures\n","train_scentless_mayweed_dir = os.path.join(train_dir, 'Scentless Mayweed')\n","# Directory with our training shepherds purse pictures\n","train_shepherds_purse_dir = os.path.join(train_dir, 'Shepherds Purse')\n","# Directory with our training Small-flowered Cranesbill pictures\n","train_small_flowered_cranesbill_dir = os.path.join(train_dir, 'Small-flowered Cranesbill')\n","# Directory with our training Sugar beet pictures\n","train_sugar_beet_dir = os.path.join(train_dir, 'Sugar beet')\n","\n","#Validation Directory\n","# Directory with our training blackgrass pictures\n","validation_blackgrass_dir = os.path.join(validation_dir, 'Black-grass')\n","# Directory with our training charlock pictures\n","validation_charlock_dir = os.path.join(validation_dir, 'Charlock')\n","# Directory with our training cleavers pictures\n","validation_cleavers_dir = os.path.join(validation_dir, 'Cleavers')\n","# Directory with our training common chickweed pictures\n","validation_common_chickweed_dir = os.path.join(validation_dir, 'Common Chickweed')\n","# Directory with our training Common Wheat pictures\n","validation_common_wheat_dir = os.path.join(validation_dir, 'Common wheat')\n","# Directory with our training Fat Hen pictures\n","validation_fat_hen_dir = os.path.join(validation_dir, 'Fat Hen')\n","# Directory with our training Loose Silky-bent pictures\n","validation_loose_silkybent_dir = os.path.join(validation_dir, 'Loose Silky-bent')\n","# Directory with our training maize pictures\n","validation_maize_dir = os.path.join(validation_dir, 'Maize')\n","# Directory with our training scentless mayweed pictures\n","validation_scentless_mayweed_dir = os.path.join(validation_dir, 'Scentless Mayweed')\n","# Directory with our training shepherds purse pictures\n","validation_shepherds_purse_dir = os.path.join(validation_dir, 'Shepherds Purse')\n","# Directory with our training Small-flowered Cranesbill pictures\n","validation_small_flowered_cranesbill_dir = os.path.join(validation_dir, 'Small-flowered Cranesbill')\n","# Directory with our training Sugar beet pictures\n","validation_sugar_beet_dir = os.path.join(validation_dir, 'Sugar beet')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S4CWhBfc6myw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":242},"outputId":"fdc7dfdc-b63b-492c-ac25-93365e8408e0","executionInfo":{"status":"ok","timestamp":1590615130635,"user_tz":300,"elapsed":18166,"user":{"displayName":"Emma K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTDcVUh3zyIi0z9xgvjNPASwgXaX71FH-FLJJSLA=s64","userId":"15689506037919733965"}}},"source":["print('total training Black-grass images:', len(os.listdir(train_blackgrass_dir)))\n","print('total training Charlock images:', len(os.listdir(train_charlock_dir))) \n","print('total training Cleavers images:', len(os.listdir(train_cleavers_dir))) \n","print('total training Common Chickweed images:', len(os.listdir(train_common_chickweed_dir)))\n","print('total training Common wheat images:', len(os.listdir(train_common_wheat_dir))) \n","print('total training Fat Hen images:', len(os.listdir(train_fat_hen_dir))) \n","print('total training Loose Silky-bent images:', len(os.listdir(train_loose_silkybent_dir))) \n","print('total training Maize images:', len(os.listdir(train_maize_dir))) \n","print('total training Scentless Mayweed images:', len(os.listdir(train_scentless_mayweed_dir))) \n","print('total training Shepherds Purse images:', len(os.listdir(train_shepherds_purse_dir))) \n","print('total training Small-flowered Cranesbill  images:', len(os.listdir(train_small_flowered_cranesbill_dir))) \n","print('total training Sugar beet images:', len(os.listdir(train_sugar_beet_dir)))\n","print('total test images:', len(os.listdir(test_dir)))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["total training Black-grass images: 263\n","total training Charlock images: 390\n","total training Cleavers images: 287\n","total training Common Chickweed images: 611\n","total training Common wheat images: 221\n","total training Fat Hen images: 475\n","total training Loose Silky-bent images: 654\n","total training Maize images: 221\n","total training Scentless Mayweed images: 516\n","total training Shepherds Purse images: 231\n","total training Small-flowered Cranesbill  images: 496\n","total training Sugar beet images: 385\n","total test images: 794\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1-6T-CCw6zyY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9c21faca-d902-4f0d-cf76-577022eb6020","executionInfo":{"status":"ok","timestamp":1590615133126,"user_tz":300,"elapsed":968,"user":{"displayName":"Emma K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTDcVUh3zyIi0z9xgvjNPASwgXaX71FH-FLJJSLA=s64","userId":"15689506037919733965"}}},"source":["TrainImg = np.asarray(train_blackgrass_dir)\n","print(TrainImg)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/AIandBigData/TP4P3/plants/train/Black-grass\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v5C4yc9I-TvN","colab_type":"text"},"source":["Performing Data Augmentation and Classification on Train: "]},{"cell_type":"code","metadata":{"id":"pjxVwqVl895j","colab_type":"code","colab":{}},"source":["#Data Augmentation\n","batch_size = 32\n","\n","train_datagen = ImageDataGenerator(\n","    rescale = 1./255, #Normalization \n","    rotation_range = 40,\n","    width_shift_range = 0.2,\n","    height_shift_range = 0.2,\n","    shear_range = 0.2,\n","    zoom_range = 0.2,\n","    horizontal_flip = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VNgnB9XA8YLK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"d9a48718-03fe-413f-eea5-b311763f1256","executionInfo":{"status":"ok","timestamp":1590615153901,"user_tz":300,"elapsed":16894,"user":{"displayName":"Emma K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTDcVUh3zyIi0z9xgvjNPASwgXaX71FH-FLJJSLA=s64","userId":"15689506037919733965"}}},"source":["#Classification\n","train_generator = train_datagen.flow_from_directory(\n","        train_dir,\n","        target_size = (150, 150), #Images resized to 150x150\n","        batch_size = batch_size,\n","        class_mode = 'categorical') #Binary Labels for Categorical Data (should result in 12 categories due to 12 different plants)\n","\n","for data_batch, labels_batch in train_generator:\n","    print('Train Data Batch Shape:', data_batch.shape)\n","    print('Train Labels Batch Shape:', labels_batch.shape)\n","    break"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Found 4750 images belonging to 12 classes.\n","Train Data Batch Shape: (32, 150, 150, 3)\n","Train Labels Batch Shape: (32, 12)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TfvAT4TR_JAr","colab_type":"text"},"source":["Performing Data Augmentation and Classification on Test: \n"]},{"cell_type":"code","metadata":{"id":"om0Rmh5b_u3n","colab_type":"code","colab":{}},"source":["#Data Augmentation \n","test_datagen = ImageDataGenerator(rescale = 1./255) #Normalization"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ubco632K_kwX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"outputId":"d7fd63b9-391d-4ed7-97cb-5758d164c84a","executionInfo":{"status":"ok","timestamp":1590615179306,"user_tz":300,"elapsed":873,"user":{"displayName":"Emma K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTDcVUh3zyIi0z9xgvjNPASwgXaX71FH-FLJJSLA=s64","userId":"15689506037919733965"}}},"source":["print('total test images:', len(os.listdir(test_dir)))\n","#Classification\n","test_generator = test_datagen.flow_from_directory(\n","        test_dir,\n","        target_size = (150, 150), #Resizes to 150x150\n","        batch_size = batch_size,\n","        class_mode = 'categorical') #Binary Labels for Categorical Data (should result in 12 categories due to 12 different plants)\n","\n","for data_batch, labels_batch in test_generator:\n","    print('Test Data Batch Shape:', data_batch.shape)\n","    print('Test Labels Batch Shape:', labels_batch.shape)\n","    break"],"execution_count":13,"outputs":[{"output_type":"stream","text":["total test images: 794\n","Found 0 images belonging to 0 classes.\n","Test Data Batch Shape: (0, 150, 150, 3)\n","Test Labels Batch Shape: (0, 0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7W3GhUcPAR6U","colab_type":"code","colab":{}},"source":["# Our sequential model using CNN\n","model = models.Sequential()\n","\n","model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n","model.add(layers.MaxPooling2D((2, 2)))\n","\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","\n","model.add(layers.Flatten())\n","\n","model.add(layers.Dropout(0.5)) # to avoid any overfitting \n","\n","model.add(layers.Dense(512, activation='relu'))\n","\n","model.add(layers.Dense(12, activation='sigmoid')) # for each categories"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpFb2GGFCRJf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":571},"outputId":"5365b9e3-201e-4802-9cd9-1c47862204af","executionInfo":{"status":"ok","timestamp":1590615191718,"user_tz":300,"elapsed":350,"user":{"displayName":"Emma K","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTDcVUh3zyIi0z9xgvjNPASwgXaX71FH-FLJJSLA=s64","userId":"15689506037919733965"}}},"source":["# We use categorical_crossentropy as loos and same optimizer \n","model.compile(loss = 'categorical_crossentropy',\n","              optimizer = optimizers.RMSprop(lr=1e-4),\n","              metrics = ['acc'])\n","\n","model.summary()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 6272)              0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 6272)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               3211776   \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 12)                6156      \n","=================================================================\n","Total params: 3,458,764\n","Trainable params: 3,458,764\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ak8VddobCtF6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":415},"outputId":"10c7f257-0430-4631-bab4-9e4d378e9faf"},"source":["history = model.fit_generator(\n","      train_generator,\n","      steps_per_epoch = 10,\n","      epochs = 30)\n","\n","model.save('Plants.h5')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","10/10 [==============================] - 151s 15s/step - loss: 2.4490 - acc: 0.1291\n","Epoch 2/30\n","10/10 [==============================] - 152s 15s/step - loss: 2.4240 - acc: 0.1156\n","Epoch 3/30\n","10/10 [==============================] - 153s 15s/step - loss: 2.4222 - acc: 0.1469\n","Epoch 4/30\n","10/10 [==============================] - 161s 16s/step - loss: 2.4218 - acc: 0.1719\n","Epoch 5/30\n","10/10 [==============================] - 147s 15s/step - loss: 2.4034 - acc: 0.1469\n","Epoch 6/30\n","10/10 [==============================] - 150s 15s/step - loss: 2.4531 - acc: 0.1156\n","Epoch 7/30\n","10/10 [==============================] - 157s 16s/step - loss: 2.4318 - acc: 0.1094\n","Epoch 8/30\n","10/10 [==============================] - 154s 15s/step - loss: 2.4178 - acc: 0.1344\n","Epoch 9/30\n","10/10 [==============================] - 153s 15s/step - loss: 2.4400 - acc: 0.1250\n","Epoch 10/30\n","10/10 [==============================] - 156s 16s/step - loss: 2.4309 - acc: 0.1375\n","Epoch 11/30\n","10/10 [==============================] - 144s 14s/step - loss: 2.4366 - acc: 0.1250\n","Epoch 12/30\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CHfJpoF2Cw-4","colab_type":"code","colab":{}},"source":["acc = history.history['acc']\n","loss = history.history['loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, label = 'Training acc')\n","plt.title('Training accuracy')\n","plt.legend()\n","plt.figure()\n","\n","plt.plot(epochs, loss, label = 'Training loss')\n","plt.title('Training  loss')\n","plt.legend()\n","plt.figure()\n","\n","plt.show()"],"execution_count":0,"outputs":[]}]}