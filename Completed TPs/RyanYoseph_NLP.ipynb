{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TP_NLP_sujet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "72tQHNlEVe33"
      },
      "source": [
        "# TP NLP \n",
        "\n",
        "This lab is based on the NLP lab from Ecole polytechnique.\n",
        "\n",
        "## 0. Words embedding\n",
        "\n",
        "In this lab, you will discover words embedding through several tasks such as translation or classification. The first step will be to download pretrained embeddings. To do so, you have to run the following cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p5hHp3d5bZDY",
        "outputId": "b8b4d2e8-d084-4822-dc46-ba5a14401e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-27 01:21:18--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.205.181\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.205.181|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  13.6MB/s    in 1m 57s  \n",
            "\n",
            "2020-05-27 01:23:17 (13.4 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6HIGtxxcVYPX",
        "colab": {}
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IEoX84_2ZQOh",
        "outputId": "9fcea50e-de72-40cf-9672-e5f184035351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "\n",
        "#Run this cell only to test if you have correctedly downloaded the data.\n",
        "\n",
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "\n",
        "#Use the following line to assert that the Word2Vec data is available\n",
        "print(word2vec.word_vec(\"test\")[:10])\n",
        "\n",
        "#Once you get the expected results, you don't have to run this cell again.\n",
        "del word2vec"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[-0.14257812 -0.03686523  0.13574219 -0.06201172  0.07958984  0.01904297\n",
            " -0.08154297 -0.12792969 -0.02954102  0.23632812]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3bU0frRc-itB"
      },
      "source": [
        "## 1. Word2Vec applied to English Vocabulary\n",
        "\n",
        "We will use this embeddings to evaluate similarities between words. These tests will also help us to evaluate the quality of the embedding. To do so, we are going to use the given Word2Vec class. The structure of the class has been created (init and load_wordvec methods) but you'll have to complete the two following methods :\n",
        "\n",
        "*   *most_similar :* Compute the K most similar words to a given word\n",
        "\n",
        "    Input : a word w and the expected number of outputs K\n",
        "\n",
        "    Output : The K closest numbers according to the score function\n",
        "\n",
        "*   *score :* Compute the cosine similarity between two vectors \n",
        "\n",
        "    Input : two words w1 and w2\n",
        "\n",
        "    Output : Cosine similarities between the two vectors corresponding to the two inputs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PzFYMhiC-oBA",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from scipy import spatial\n",
        "\n",
        "class Word2vec():\n",
        "    def __init__(self, fname):\n",
        "        self.load_wordvec(fname)\n",
        "        self.vocab = self.word2vec.vocab.keys()\n",
        "    \n",
        "    def load_wordvec(self, fname):\n",
        "        self.word2vec = KeyedVectors.load_word2vec_format(fname, binary=True, limit = 500000)\n",
        "\n",
        "    def most_similar(self, w, K=5):\n",
        "        # TODO \n",
        "        #make loop over english vocabulary (self.vocab) compute score call score\n",
        "\n",
        "        closest_numbers = [0 for i in range(K+1)]\n",
        "        words = [0 for i in range(K+1)]\n",
        "\n",
        "        for iterLoop in range(K+1):\n",
        "          for keys in self.vocab:\n",
        "            s = self.score(w, keys)\n",
        "            if iterLoop == 0:\n",
        "              if (s>closest_numbers[iterLoop]):\n",
        "                closest_numbers[iterLoop] = s\n",
        "                words[iterLoop] = keys, closest_numbers[iterLoop]\n",
        "            else:\n",
        "              if (s>closest_numbers[iterLoop] and s<closest_numbers[iterLoop-1]):\n",
        "                closest_numbers[iterLoop] =  s\n",
        "                words[iterLoop] = keys, closest_numbers[iterLoop]\n",
        "\n",
        "        del words[0] #it will find itself, so we disregard that\n",
        "\n",
        "        return words\n",
        "    def score(self, w1, w2):\n",
        "        # TODO (help : np.linalg.norm will return the norm of a vector) cosine simularity\n",
        "\n",
        "        vec1 = self.word2vec.word_vec(w1)\n",
        "        # print(w1, \": \", vec1)\n",
        "        vec2 = self.word2vec.word_vec(w2)\n",
        "\n",
        "        cos_sim = (np.sum(vec1*vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2)))\n",
        "        \n",
        "        return cos_sim\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LarugNtxAdOa",
        "outputId": "d9188ae0-1d37-4741-fbf6-bb6650955075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "w2v = Word2vec(EMBEDDING_FILE)\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VrkiNgyJO9x9"
      },
      "source": [
        "Run the following cell to evaluate your model. You don't have to modify this cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DIAEV3qJODmp",
        "outputId": "aff4d243-f4f2-4421-86e1-79f00c488cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# You will be evaluated on the output of the following:\n",
        "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
        "    print(w1, w2, w2v.score(w1, w2))\n",
        "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
        "    print(w2v.most_similar(w1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat dog 0.76094574\n",
            "dog pet 0.71647847\n",
            "dogs cats 0.7651765\n",
            "paris france 0.5550795\n",
            "germany berlin 0.5539934\n",
            "[('cats', 0.80993795), ('dog', 0.76094574), ('kitten', 0.74649847), ('feline', 0.7326235), ('beagle', 0.7150583)]\n",
            "[('dogs', 0.868049), ('puppy', 0.81064284), ('pit_bull', 0.780396), ('pooch', 0.76273763), ('cat', 0.76094574)]\n",
            "[('dog', 0.868049), ('canines', 0.8181711), ('cats', 0.7651765), ('pit_bulls', 0.75483024), ('pets', 0.7424417)]\n",
            "[('london', 0.5555775), ('france', 0.5550795), ('dubai', 0.5532332), ('rome', 0.5465838), ('toronto', 0.5457153)]\n",
            "[('german', 0.6809574), ('europe', 0.6781216), ('european', 0.65021104), ('sweden', 0.638424), ('france', 0.63143545)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TszfB41eTkds"
      },
      "source": [
        "You can compare your results with the method *most_similar_cosmul* that uses another solution to compute the most similar words :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OeMzXAgORkW_",
        "outputId": "9ae35d2a-b32a-42b3-a717-ccd103d18de7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
        "  print(w2v.word2vec.most_similar_cosmul(w1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('cats', 0.9049681425094604), ('dog', 0.8804720044136047), ('kitten', 0.8732484579086304), ('feline', 0.8663108348846436), ('beagle', 0.8575283288955688), ('puppy', 0.8537718653678894), ('pup', 0.8467137217521667), ('pet', 0.8445757627487183), ('felines', 0.8377957940101624), ('chihuahua', 0.8354873061180115)]\n",
            "[('dogs', 0.9340235590934753), ('puppy', 0.9053205847740173), ('pit_bull', 0.8901971578598022), ('pooch', 0.8813680410385132), ('cat', 0.8804720044136047), ('golden_retriever', 0.8750442862510681), ('German_shepherd', 0.8732578754425049), ('Rottweiler', 0.8718799352645874), ('beagle', 0.8709302544593811), ('pup', 0.8703446984291077)]\n",
            "[('dog', 0.9340235590934753), ('canines', 0.9090846180915833), ('cats', 0.8825874328613281), ('pit_bulls', 0.8774142861366272), ('pets', 0.8712201118469238), ('puppies', 0.8692987561225891), ('pooches', 0.8581174612045288), ('German_shepherds', 0.8535523414611816), ('animals', 0.8492838740348816), ('pit_bull', 0.8491798639297485)]\n",
            "[('london', 0.7777880430221558), ('france', 0.7775390148162842), ('dubai', 0.7766158580780029), ('rome', 0.7732911705970764), ('toronto', 0.7728569507598877), ('las_vegas', 0.7720862627029419), ('spain', 0.7690009474754333), ('berlin', 0.762671947479248), ('michelle', 0.7621249556541443), ('elle', 0.761517345905304)]\n",
            "[('german', 0.8404779434204102), ('europe', 0.8390600681304932), ('european', 0.8251047730445862), ('sweden', 0.8192111849784851), ('france', 0.8157169222831726), ('spain', 0.8143780827522278), ('russia', 0.8092007040977478), ('america', 0.8038331866264343), ('usa', 0.8033005595207214), ('india', 0.8012795448303223)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ZmJ0g60xf3",
        "colab_type": "text"
      },
      "source": [
        "The first cell (the one not with the method most_similiar_cosmul) is less accruate by about 0.1-0.15 than the second cell. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PzI3DWdvT7-a"
      },
      "source": [
        "## 2. Bag of Words\n",
        "\n",
        "A quick solution to implement in NLP is Bag of Words. This means that we are going to consider the whole sentences without any word order. We are going to compute two solutions, one with idf and one without idf. \n",
        "\n",
        "First, we need to download the dataset. We are going to use the dataset from IMDB based on movie reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IZqTk7LcVGg_",
        "outputId": "ff03f0dc-4bba-43e8-97ad-f2a225521fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.datasets import imdb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eJuWViAhXS2B",
        "outputId": "7286c02c-af9f-4a5e-8d67-eaf14d39185a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=100000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pUqhdc3bYG1t",
        "outputId": "277544fc-a3fb-45ef-ef4f-2e8e67b0ea7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "INDEX_FROM = 3\n",
        "word_to_id = imdb.get_word_index()\n",
        "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "word_to_id[\"<UNUSED>\"] = 3\n",
        "\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "print(' '.join(id_to_word[id] for id in x_train[0] if id > 2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n",
            "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "503TEXrWaHak",
        "outputId": "1c7f96c9-75cc-438e-ac73-56f547497223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "truncated_sentences = []\n",
        "\n",
        "for sent_ind in range(1000):\n",
        "  sentence = [id_to_word[id] for id in x_train[sent_ind] if id > 2]\n",
        "  truncated_sentences.append(sentence[:15])\n",
        "\n",
        "print(truncated_sentences[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this', 'film', 'was', 'just', 'brilliant', 'casting', 'location', 'scenery', 'story', 'direction', \"everyone's\", 'really', 'suited', 'the', 'part']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xZtSrzGsQMYV"
      },
      "source": [
        "You will have to complete the following class. The methods are given but you'll have to complete them.\n",
        "\n",
        "*   *encode* : Encode the sentences with a Bag of Words algorithm using a simple mean or using an idf-weighted mean\n",
        "\n",
        "  Input : sentences to encode, a boolean defining if we are goinf to use the idf-weighted method.\n",
        "\n",
        "  Output : A matrix with the sentences embedding\n",
        "\n",
        "*   *most_similar* : Find the K most similar sentences in the corpus.\n",
        "\n",
        "  Input : a sentence s that need to be matched, a dataset of sentences, the idf boolean for the method and the number K of sentences to return\n",
        "\n",
        "  Output : The K most similar sentences\n",
        "\n",
        "*   *score :* Compute the cosine similarity between two vectors \n",
        "\n",
        "    Input : two words w1 and w2\n",
        "\n",
        "    Output : Cosine similarities between the two vectors corresponding to the two inputs\n",
        "\n",
        "*   *build_idf* : Compute an idf dictionnary with all the weights\n",
        "\n",
        "  $$idf(word, corpus) =  max(1, log_{10}(\\frac{length(sentences)}{count(word)}))$$\n",
        "\n",
        "    Input : Sentences from the corpus\n",
        "\n",
        "    Output : A dictionnary with words as keys and weights as values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6GicNUSNT9fc",
        "colab": {}
      },
      "source": [
        "import statistics\n",
        "\n",
        "def sortSecond(val): \n",
        "    return val[1] \n",
        "\n",
        "class BoV():\n",
        "    def __init__(self, w2v):\n",
        "        self.w2v = w2v\n",
        "    def encode(self, sentences, idf=False):\n",
        "\n",
        "        sentemb = []  \n",
        "        for sent in sentences:\n",
        "          mean_vector = []\n",
        "          idf_vector = []\n",
        "          #index = 0  \n",
        "\n",
        "          if idf is False:\n",
        "            \n",
        "            for word in sent:\n",
        "              \n",
        "              #print(word)\n",
        "              if word in word2vec.vocab:\n",
        "                word_embed = word2vec.word_vec(word)\n",
        "                mean_vector = mean_vector + [word_embed]\n",
        "              else:\n",
        "                if len(mean_vector) == 0:\n",
        "                  mean_vector = mean_vector + [0]\n",
        "            \n",
        "            sentemb.append(sum(mean_vector)/len(mean_vector))\n",
        "            \n",
        "            \n",
        "          else:\n",
        "            \n",
        "            for word in sent:\n",
        "              if word in word2vec.vocab:\n",
        "                word_embed = word2vec.word_vec(word)\n",
        "                idf_vector = idf_vector + [idf[word.lower()]*(sum(word_embed)/len(word_embed))]\n",
        "              else:\n",
        "                idf_vector = idf_vector + [idf[word.lower()]]\n",
        "            \n",
        "            sentemb.append(sum(idf_vector)/len(idf_vector))\n",
        "         \n",
        "        return np.vstack(sentemb)\n",
        "\n",
        "    def most_similar(self, s, sentences, idf=False, K=5):\n",
        "        keys = self.encode(sentences, idf)\n",
        "        query = self.encode([s], idf)\n",
        "\n",
        "        score_tab = []\n",
        "        i = 0\n",
        "        for key in keys:\n",
        "\n",
        "          score_tab.append((sentences[i], self.score(query,key)))\n",
        "          i = i + 1\n",
        "        \n",
        "        score_tab.sort(key = sortSecond, reverse=True)\n",
        "        \n",
        "        return score_tab[:K]\n",
        "\n",
        "    def score(self, s1, s2, idf=False):\n",
        "\n",
        "        cos_sim = np.dot(s1, s2)/np.linalg.norm(s1) * np.linalg.norm(list(s2))\n",
        "        return cos_sim\n",
        "    \n",
        "    def build_idf(self, sentences): #for each word, calculate the max of 1log and so on, compute the encode, then most_similar and score\n",
        "        idf = {}\n",
        "\n",
        "        for sent in sentences:\n",
        "          for word in sent:\n",
        "            word_count = 0\n",
        "            target_word = word\n",
        "            for sent2 in sentences:\n",
        "              word_count = word_count + sent2.count(target_word)\n",
        "            idf[target_word] = np.maximum(1, np.log10(len(sentences)/word_count))\n",
        "    \n",
        "        return idf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0kUyVOoQZgTx",
        "colab": {}
      },
      "source": [
        "s2v = BoV(w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dhKszZreZgBo",
        "colab": {}
      },
      "source": [
        "idf = s2v.build_idf(truncated_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "20WZNn-qRdi1"
      },
      "source": [
        "Run the following cell and comment your result. Is that the output you expected ? To what extent, the similar sentence is good enough according to you ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS4L_RX2tbO5",
        "colab_type": "text"
      },
      "source": [
        "This output is not what I expected, seeing as the first few sentences do not seem to be condemning the movie. Idf appears to work much better than non idf, with idf at least producing some negative critiques of the movie. The output is thus not exactly what I expected for non idf, but for idf it does appear to be good enough. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gS1vfwXbapRj",
        "outputId": "f824d011-bcaf-41ea-ca34-e799151db951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#You will be evaluated on the output of the following lines. You need to print the 5 most_similar sentences.\n",
        "most_similar_sentences = s2v.most_similar(\"This was the worst movie I ever seen in my life\".split(\" \"), truncated_sentences)\n",
        "print(most_similar_sentences)\n",
        "most_similar_sentences_idf = s2v.most_similar(\"This was the worst movie I ever seen in my life\".split(\" \"), truncated_sentences, idf)\n",
        "print(most_similar_sentences_idf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['oh', 'my', 'god', 'i', 'bought', 'this', 'movie', 'and', 'i', 'watched', 'the', 'whole', 'thing', 'okay', \"it's\"], array([1.2397451], dtype=float32)), (['wow', 'what', 'would', 'you', 'do', 'with', '33m', 'let', 'me', 'give', 'you', 'a', 'choice', 'you', 'can'], array([1.1543801], dtype=float32)), (['i', 'think', 'that', 'this', 'movie', 'is', 'very', 'fun', 'and', 'horror', 'i', 'love', 'elvira', 'and', 'i'], array([1.1539967], dtype=float32)), (['this', 'movie', 'is', 'terrible', 'terrible', 'one', 'of', 'the', 'worst', 'movies', 'ever', 'i', 'cannot', 'even', 'imagine'], array([1.1418922], dtype=float32)), (['i', 'loved', 'this', 'movie', 'i', 'am', 'biased', 'seeing', 'as', 'i', 'am', 'a', 'huge', 'disney', 'fan'], array([1.1119549], dtype=float32))]\n",
            "[(['the', 'brak', 'show', 'is', 'good', 'probably', 'not', 'in', 'the', 'same', 'level', 'than', 'aqua', 'teen', 'hunger'], array([0.00019243])), (['netflix', 'should', 'mention', 'this', 'short', 'feature', 'on', 'the', 'info', 'for', 'silk', 'stockings', 'superior', 'in', 'every'], array([0.0001303])), (['the', 'scots', 'excel', 'at', 'storytelling', 'the', 'traditional', 'sort', 'many', 'years', 'after', 'the', 'event', 'i', 'can'], array([0.00012273])), (['this', 'show', 'is', 'awful', 'no', 'comedy', 'no', 'plot', 'no', 'good', 'characters', 'america', 'are', 'you', 'blind'], array([9.34519617e-05])), (['i', 'cant', 'believe', 'some', 'people', 'actually', 'like', 'this', 'yet', 'still', 'call', 'themselves', 'batman', 'fans', 'even'], array([9.25727843e-05]))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8ZGG6GRKnzf7"
      },
      "source": [
        "## 3. Translation\n",
        "\n",
        "One task that you can expect from an NLP application is to compute a quick translation. Creating advanced tools for translation is hard but there is an easy way to translate. We are going to compute a quick translation thanks to projection. We start from the assumption that the two languages have more or less the same shape. So with a simple mapping we should be able to translate automatically.\n",
        "\n",
        "You can visualise it with the gif from [here](https://engineering.fb.com/ai-research/unsupervised-machine-translation-a-novel-approach-to-provide-fast-accurate-translations-for-more-languages/). \n",
        "\n",
        "### Mapping\n",
        "\n",
        "Let's consider a bilingual dictionary (e.g French-English).\n",
        "\n",
        "Let's define **X** and **Y** the **French** and **English** matrices.\n",
        "\n",
        "They contain the embeddings associated to the words in the bilingual dictionary.\n",
        "\n",
        "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
        "\n",
        "We want to find : $$W^* = argmin || W.X - Y || \\text{ such that } W^T.W = Id$$\n",
        "\n",
        "Fortunately, the problem has a closed form solution:\n",
        "$$W = U.V^T  \\text{ where }  U.\\Sigma.V^T = SVD(Y.X^T)$$\n",
        "\n",
        "\n",
        "First we need to download the data. Because of the limited RAM in google colab, we are going to only use the part of the model that we need. You have to run the following cells. It will take some time.\n",
        "\n",
        "WARNING : Bedore running the following cells, you should restart the the environnement to assert that your RAM is empty. It will clear the variables from the previous part. Otherwise, you might exceed the limitation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kNMgEqf496Gh",
        "outputId": "f4e646fd-64d6-4bd4-f3aa-8ba55e1d38da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!wget -P /root/input/ -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz\n",
        "!wget -P /root/input/ -c https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-27 19:35:57--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4a8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "--2020-05-27 19:36:00--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4a8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFsrRdSwtQLz",
        "colab_type": "text"
      },
      "source": [
        "Run time taking forever for unknown reasons. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NKEh-gdJAl8C",
        "outputId": "97e160e1-e0a8-4461-ca0f-263ea8507b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "!gunzip -k /root/input/cc.fr.300.bin.gz\n",
        "!gunzip -k /root/input/cc.en.300.bin.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gzip: /root/input/cc.fr.300.bin already exists; do you wish to overwrite (y or n)? y\n",
            "gzip: /root/input/cc.en.300.bin already exists; do you wish to overwrite (y or n)? "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a54f89786d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gunzip -k /root/input/cc.fr.300.bin.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gunzip -k /root/input/cc.en.300.bin.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eyywOx-PHe96",
        "colab": {}
      },
      "source": [
        "import gensim.models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vZ1De7OLSk_E",
        "colab": {}
      },
      "source": [
        "ft = gensim.models.FastText.load_fasttext_format('/root/input/cc.fr.300.bin')  # Original fasttext embeddings from https://fasttext.cc/\n",
        "ft.wv.save('/root/input/gensim_fasttext_fr.model')\n",
        "#del ft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wYMPFA5YHuUe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "c8d51850-0417-4505-81d3-bdef77b9cf82"
      },
      "source": [
        "ft = gensim.models.FastText.load_fasttext_format('/root/input/cc.en.300.bin')  # Original fasttext embeddings from https://fasttext.cc/\n",
        "ft.wv.save('/root/input/gensim_fasttext_en.model')\n",
        "#del ft"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8a9397766639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_fasttext_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/root/input/cc.en.300.bin'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Original fasttext embeddings from https://fasttext.cc/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/root/input/gensim_fasttext_en.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#del ft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5kDc0gSW7nqr",
        "colab": {}
      },
      "source": [
        "ft_french = gensim.models.KeyedVectors.load('/root/input/gensim_fasttext_fr.model')\n",
        "ft_english = gensim.models.KeyedVectors.load('/root/input/gensim_fasttext_en.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H40m3SbHYQFI"
      },
      "source": [
        "You can now test your model contained in ft_french and ft_english."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "51J115OMAOH5",
        "colab": {}
      },
      "source": [
        "#You can test if your model is working\n",
        "\n",
        "#print(ft_french.words[:10])   # list of words in dictionary\n",
        "print(ft_french['roi']) # get the vector of the word 'king'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DHU3m-08YxaE"
      },
      "source": [
        "Now we can start the translation. The first step is to define the X and Y matrix. Instead of computing the whole vocabulary, we are going to use a trick. We need to assert that the words are in the same position in X and Y. \n",
        "\n",
        "For the example if \"dog\" is the 100th word in X, \"chien\" should be the 100th word. To do so, we are going to use transparent words only. We have the strong assumption that if a word exists in both languages it should have the same meaning in both of them.\n",
        "\n",
        "Compute the intersection of the two vocabulary and store the first **20000 common words** with their embeddings in X and Y. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAbJM5rCV0kA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = {}\n",
        "Y = {}\n",
        "\n",
        "#TODO\n",
        "count=0;\n",
        "condition=False\n",
        "for fw in ft_french.vocab:\n",
        "  if condition:\n",
        "    break;\n",
        "  for ew in ft_english.vocab:\n",
        "    if fw==ew:\n",
        "      X[fw]=ft_french[fw];\n",
        "      Y[ew]=ft_english[ew];\n",
        "      count+=1;  \n",
        "      if count==20000: #20,000 most common words\n",
        "        condition=True\n",
        "        break;\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yljm760SZzzQ"
      },
      "source": [
        "Now compute W with the given close form. The shape of W should be (300, 300)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TDejtYtG8A-O",
        "colab": {}
      },
      "source": [
        "import scipy.linalg\n",
        "import numpy as np\n",
        "\n",
        "#TODO\n",
        "Xmat=[]\n",
        "Ymat=[]\n",
        "for x in X.values():\n",
        "    Xmat.append(x)\n",
        "for y in Y.values():\n",
        "    Ymat.append(y)\n",
        "\n",
        "Xmat= np.array(Xmat)\n",
        "Ymat= np.array(Ymat)\n",
        "\n",
        "print(Xmat.shape)\n",
        "print(Ymat.shape)\n",
        "\n",
        "u,s,wt = np.linalg.svd(np.dot(Ymat,Xmat.T),full_matrices=False) \n",
        "W = np.dot(u,wt)\n",
        "\n",
        "W.resize(300, 300) #resize to 300,300\n",
        "print(W.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TStieykhQ77Z"
      },
      "source": [
        "Run the following cell and comment your result. Is that the output you expected ? Why ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S9LysDfIQuU4",
        "colab": {}
      },
      "source": [
        "K = 5\n",
        "\n",
        "#You will be evaluated on the following outputs\n",
        "\n",
        "def score(value1, value2):\n",
        "  return np.dot(value1, value2)/(np.linalg.norm(value1)*np.linalg.norm(value2))\n",
        "\n",
        "print(\"French to English : \")\n",
        "print()\n",
        "for w1 in ['souris', 'travailler', 'rue', 'pays']:\n",
        "    # Compute the projection of each word and find the 5 closest word in the English vocabulary\n",
        "    print()#Print the list of the 5 best outputs for each word\n",
        "\n",
        "print()\n",
        "print(\"English to French : \")\n",
        "for w2 in ['cat', 'target', 'city', 'free']:\n",
        "    # Compute the projection of each word and find the 5 closest word in the French vocabulary\n",
        "    print()#Print the list of the 5 best outputs for each word\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eCSdbk3YaRBI"
      },
      "source": [
        "WARNING : When you have finished the previous part, you can delete the two model to free the RAM before the next part. Don't delete them before you have finished the previous question, otherwise, you'll have to recompute all the downloading part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TNwgBJ5-SMpC",
        "colab": {}
      },
      "source": [
        "del ft_french\n",
        "del ft_english"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G9ta4JGrWC52"
      },
      "source": [
        "## 4. Classification\n",
        "\n",
        "We are now starting the Deep Learning part with a classification task. We have a dataset from IMDB with movie reviews. We are going to build a simple classifier. The objective is to make a binary classifier to separate positive from negative reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CA6nKkjVa91w",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=top_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-FOAzoTccxGo",
        "colab": {}
      },
      "source": [
        "max_review_length = 500\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2x2b3LvLd8rd"
      },
      "source": [
        "The first model that you need to compute is a simple feed forward neural network.\n",
        "\n",
        "- Use the following layers to build your network\n",
        "  - An embedding layer with top_words = 5000, an embedding vector length of 32 and a max reviex length of 500\n",
        "  - A fully-connected layer with 16 units and a ReLU activiation function\n",
        "  - An output fully-connected layer with one unit and a Sigmoid activation function.\n",
        "- Compile the generator using binary crossentropy loss and the Adam optimizer \n",
        "- Train on the data with 20 epochs\n",
        "- Compute the accuracy with model.evaluate\n",
        "- Plot the history of the loss versus the epoch number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tI-2p_6scxB4",
        "outputId": "d1718791-e65d-4c2a-c9f6-74cbd061775f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(5000, 32, input_length = 500))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=20,\n",
        ")\n",
        "\n",
        "results = model.evaluate(X_test, y_test, batch_size=128)\n",
        "print('test loss, test acc:', results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6892 - accuracy: 0.5194\n",
            "Epoch 2/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6878 - accuracy: 0.5235\n",
            "Epoch 3/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6877 - accuracy: 0.5216\n",
            "Epoch 4/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6877 - accuracy: 0.5227\n",
            "Epoch 5/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6876 - accuracy: 0.5213\n",
            "Epoch 6/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6876 - accuracy: 0.5221\n",
            "Epoch 7/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6876 - accuracy: 0.5245\n",
            "Epoch 8/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6876 - accuracy: 0.5205\n",
            "Epoch 9/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6875 - accuracy: 0.5220\n",
            "Epoch 10/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6876 - accuracy: 0.5243\n",
            "Epoch 11/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6876 - accuracy: 0.5245\n",
            "Epoch 12/20\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.6875 - accuracy: 0.5234\n",
            "Epoch 13/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6876 - accuracy: 0.5231\n",
            "Epoch 14/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6875 - accuracy: 0.5228\n",
            "Epoch 15/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6876 - accuracy: 0.5238\n",
            "Epoch 16/20\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.6875 - accuracy: 0.5236\n",
            "Epoch 17/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6875 - accuracy: 0.5207\n",
            "Epoch 18/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6875 - accuracy: 0.5243\n",
            "Epoch 19/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6875 - accuracy: 0.5226\n",
            "Epoch 20/20\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.6875 - accuracy: 0.5211\n",
            "196/196 [==============================] - 1s 5ms/step - loss: 0.6885 - accuracy: 0.5196\n",
            "test loss, test acc: [0.6885230541229248, 0.5196367502212524]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4_lkYQvQ_Kp",
        "colab_type": "code",
        "outputId": "46c8fd0e-3628-4d46-a683-a5a33028371d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.title('Training Loss x Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de5QX9X3/8edLlkvwioJGQblEUNAgxNUEKEKwGkzyU5OmBkobjYmojTUxrQbCSUPoL+ck5lL1V9IGL41psGJJazEnSjRqvGJYFCHcFBBwCUGCBOXgBfD9+2M+C8PXXXZ2v7v73cXX45w5+53PfGbmM7Pfnfd+LjOjiMDMzKyIQypdADMz6zgcNMzMrDAHDTMzK8xBw8zMCnPQMDOzwhw0zMysMAcN65Ak3S/p0pbOa+WT1E9SSKqqdFms5cn3aVhbkbQjN9sdeAvYk+avjIjZbV+q5pM0FvhZRPSpdFkORFIAO4H8H/uMiLixlfbXD3gJ6BwRu1tjH1Y5/k/A2kxEHFb3WdI64IsR8VBpPklVvti0uDMiYnWlC2Edn5unrOIkjZVUK+lrkv4A/LukHpJ+IWmLpG3pc5/cOo9K+mL6fJmkJyR9P+V9SdIFzczbX9Jjkl6X9JCkmZJ+1oxjGpz2+ydJyyRdmFv2cUnL0z42SvqHlN4zHeefJL0q6XFJ7/oblTRS0h8lnZjmz0jHcmozyjld0lxJc1J5npV0RsHjeJ+kH0haL2l7Oq/vy21+kqQNqazTcuudLalG0muSNkv6YVPLbZXjoGHtxfuBo4G+wGSy7+a/p/mTgDeAfznA+h8GVgE9gRuB2yWpGXnvAn4LHANMB/6mqQciqTNwH/Ar4Fjg74DZkk5JWW4na447HDgdeDil/z1QC/QCjgO+zv5NSgBExFPAj4E700X6Z8A3ImJlU8uaXAT8F9n5vwu4V1LnAsfxfeBMYGRa9wbgndx2/ww4BTgX+EdJg1P6zcDNEXEE8AHgnmaW2yrAQcPai3eAb0bEWxHxRkRsjYifR8TOiHgd+DYw5gDrr4+IWyNiD3AncDzZhbdwXkknAWcB/xgRb0fEE8C8ZhzLR4DDgO+k7TwM/AKYmJbvAoZIOiIitkXEs7n044G+EbErIh6PhjsdpwNHkgW4jcDMRsr0bKot1E0fyy1bFBFzI2IX8EOgWzqGBo8j1YAuB74cERsjYk9EPBURb+W2+630u3weeB6oq8HsAk6W1DMidkTEgkbKbu2Ig4a1F1si4s26GUndJf04NX28BjwGHCWpUwPr/6HuQ0TsTB8Pa2LeE4BXc2kALzfxOEjbeTki8v91rwd6p89/AXwcWC/pN5JGpPTvAauBX0laK2lKQztIF/ifkNVUfnCA4FLnQxFxVG6an1u29xhTmWvTMRzoOHqSBZc1B9jnH3Kfd7Lv9/EFYBCwUtJCSZ9spOzWjjhoWHtRetH7e7KmjQ+nZoxzUnpDTU4tYRNwtKTuubQTm7Gd3wMnlvRHnERWIyAiFkbERWRNPveSmmci4vWI+PuIGABcCHxV0rn17UBSb+CbZE14P5DUtRnlrLP3GFOZ+6RjONBx/BF4k6x5qUki4sWImEh2/N8F5ko6tPnFt7bkoGHt1eFk/Rh/knQ02QWyVUXEeqAGmC6pS6oB/J/G1pPULT+RNRntBG5IfQNj03buTtudJOnIVFt4jdQPIOmTkk5O/SvbyYYjv1PP/kRWy7id7L/2TcA/lXHoZ0r6tLL7Kr5CNhR6AfBMQ8eRah93AD+UdIKkTpJGFAlekv5aUq+0jT+l5Hcdp7VPDhrWXt0EvI/sP9oFwANttN9JwAhgK/B/gTlkF9GG9CYLbvnpRLKL6wVk5f8R8LlcR/XfAOtSs9tVaZ8AA4GHgB3A08CPIuKRevZ5Ldl/6d9IzVKfBz4vafQByvm8pB256abcsv8FPgtsS2X7dOpTebuR4/gHYCmwEHiVrNZQ5JoyHlim7L6dm4EJEfFGgfWsHfDNfWYHIGkOsDIiWr2mUwmSpgMnR8RfV7os1jG4pmGWI+ksSR+QdIik8WTDUe+tdLnM2gvfEW62v/cD/012n0YtcHVEPFfZIpm1H26eMjOzwtw8ZWZmhR3UzVM9e/aMfv36VboYZmYdyqJFi/4YEb3qW1YoaKQOwZuBTsBtEfGdevJcQvZogwCej4i/Suk3Ap8gq9U8SPbYgZD0WWBa2uYvIuJrKf9XgS8Cu4EtwOVp/DyS9pAN8QPYEBF7H55Wn379+lFTU1PkEM3MLJG0vqFljQaN9NiGmcB5ZB2DCyXNi4jluTwDganAqIjYJunYlD4SGAUMTVmfAMZIWkr2yIQzI2KLpDslnRsRvwaeA6ojYqekq8keKPfZtP4bETGsSUdvZmYtpkifxtnA6ohYm272uZtsGGLeFcDMiNgGEBGvpPQgez5NF6Ar0BnYDAwAXoyILSnfQ2TP4yEiHsk9+2cB2SMNzMysHSgSNHqz/0Pbatn34LU6g4BBkp6UtCA1ZxERTwOPkD3mYBMwPyJWkD2U7RRlr4WsAi6m/mf8fAG4PzffLT2Hf4Gki+srrKTJKU/Nli1b6stiZmbN1FId4VVkj0AYS1YzeEzSB8mehDmYfbWFByWNjojHU9PTHLJnzjxFyYPPJP01UM3+j8PuGxEbJQ0AHpa0NCL2e8pmRMwCZgFUV1d7PLFZO7Vr1y5qa2t58803G89sraJbt2706dOHzp07F16nSNDYyP61gD4pLa8WeCY9gO0lSS+wL4gsiIgdAJLuJ3uuz+MRcR/ZC16QNJl974pG0p+TdZKPyT+fPyLqnhK6VtKjwHAO/GhmM2unamtrOfzww+nXrx8Nvy/LWktEsHXrVmpra+nfv3/h9Yo0Ty0EBip7DWYXYALvfjHNvWQBAkk9yZqr1gIbyDq+q9JbwMYAK1K+us7yHsDfArel+eFkbyW7MNc3grLXf3bN7WMUsLczviXNng39+sEhh2Q/Z89ujb2Yvbe9+eabHHPMMQ4YFSKJY445psk1vUZrGhGxW9I1wHyy4bF3RMQySTOAmoiYl5adL2k5WY3h+ojYKmkuMI5smGwAD6QaBsDN2vcu4hkR8UL6/D2yl7X8V/oy1Q2tHQz8WNI7ZMHuO/kRXC1l9myYPBl2pq749euzeYBJkxpez8yazgGjsppz/g/qx4hUV1dHU+/T6NcvCxSl+vaFdetapFhmBqxYsYLBgwc3ntFaVX2/B0mLIqK6vvx+jEiJDRualm5mHdPWrVsZNmwYw4YN4/3vfz+9e/feO//2228fcN2amhquvfbaRvcxcuTIFinro48+yic/2T7eiuugUeKkk5qWbmZto6X7Go855hgWL17M4sWLueqqq7juuuv2znfp0oXdu3c3uG51dTW33HJLo/t46qmnyitkO+SgUeLb34bu3fdP6949Szezyqjra1y/HiL29TW29CCVyy67jKuuuooPf/jD3HDDDfz2t79lxIgRDB8+nJEjR7Jq1Spg///8p0+fzuWXX87YsWMZMGDAfsHksMMO25t/7NixfOYzn+HUU09l0qRJ1HUN/PKXv+TUU0/lzDPP5Nprr220RvHqq69y8cUXM3ToUD7ykY+wZMkSAH7zm9/srSkNHz6c119/nU2bNnHOOecwbNgwTj/9dB5//PGyz9FB/cDC5qjr7J42LWuSOumkLGC4E9yscqZN2zc4pc7OnVl6S/9t1tbW8tRTT9GpUydee+01Hn/8caqqqnjooYf4+te/zs9//vN3rbNy5UoeeeQRXn/9dU455RSuvvrqd9378Nxzz7Fs2TJOOOEERo0axZNPPkl1dTVXXnkljz32GP3792fixImNlu+b3/wmw4cP59577+Xhhx/mc5/7HIsXL+b73/8+M2fOZNSoUezYsYNu3boxa9YsPvaxjzFt2jT27NnDztKT2AwOGvWYNMlBwqw9acu+xr/8y7+kU6dOAGzfvp1LL72UF198EUns2rWr3nU+8YlP0LVrV7p27cqxxx7L5s2b6dNn/ycgnX322XvThg0bxrp16zjssMMYMGDA3vskJk6cyKxZsw5YvieeeGJv4Bo3bhxbt27ltddeY9SoUXz1q19l0qRJfPrTn6ZPnz6cddZZXH755ezatYuLL76YYcPKf3Sfm6fMrN1ry77GQw89dO/nb3zjG3z0ox/ld7/7Hffdd1+D9zR07dp17+dOnTrV2x9SJE85pkyZwm233cYbb7zBqFGjWLlyJeeccw6PPfYYvXv35rLLLuOnP/1p2ftx0DCzdq9SfY3bt2+nd+/sUXs/+clPWnz7p5xyCmvXrmVdGs8/Z86cRtcZPXo0s1NnzqOPPkrPnj054ogjWLNmDR/84Af52te+xllnncXKlStZv349xx13HFdccQVf/OIXefbZZ8sus4OGmbV7kybBrFnZ/VJS9nPWrNZvRr7hhhuYOnUqw4cPb/GaAcD73vc+fvSjHzF+/HjOPPNMDj/8cI488sgDrjN9+nQWLVrE0KFDmTJlCnfeeScAN910E6effjpDhw6lc+fOXHDBBTz66KOcccYZDB8+nDlz5vDlL3+57DL75j4zqwjf3JfZsWMHhx12GBHBl770JQYOHMh1113XZvv3zX1mZh3IrbfeyrBhwzjttNPYvn07V155ZaWLdEAePWVmVkHXXXddm9YsyuWahplVzMHcPN4RNOf8O2iYWUV069aNrVu3OnBUSN37NLp169ak9dw8ZWYV0adPH2pra/FrmSun7s19TeGgYWYV0blz5ya9Mc7aBzdPmZlZYQ4aZmZWmIOGmZkV5qBhZmaFOWiYmVlhhYKGpPGSVklaLWlKA3kukbRc0jJJd+XSb0xpKyTdIkkp/bOSlqRl383l7yppTtrXM5L65ZZNTemrJH2suQdtZmbN02jQkNQJmAlcAAwBJkoaUpJnIDAVGBURpwFfSekjgVHAUOB04CxgjKRjgO8B56b875d0btrcF4BtEXEy8M/Ad9O2hgATgNOA8cCPUtnMzKyNFKlpnA2sjoi1EfE2cDdwUUmeK4CZEbENICJeSekBdAO6AF2BzsBmYADwYkTU3dXzEPAX6fNFwJ3p81zg3FQ7uQi4OyLeioiXgNWpbGZm1kaKBI3ewMu5+dqUljcIGCTpSUkLJI0HiIingUeATWmaHxEryC74p0jqJ6kKuBg4sXR/EbEb2A4cU7AcSJosqUZSje80NTNrWS3VEV4FDATGAhOBWyUdJelkYDDQh+wCP07S6FQjuRqYAzwOrAP2tERBImJWRFRHRHWvXr1aYpNmZpYUCRob2VcLgCwAbCzJUwvMi4hdqenoBbIg8ilgQUTsiIgdwP3ACICIuC8iPhwRI4BVaZ399pdqIUcCWwuWw8zMWlGRoLEQGCipv6QuZJ3R80ry3EtWy0BST7LmqrXABrKO7ypJnYExwIqU79j0swfwt8BtaVvzgEvT588AD0f2GMx5wIQ0uqo/WVD6bZOP2MzMmq3RBxZGxG5J1wDzgU7AHRGxTNIMoCYi5qVl50taTtbMdH1EbJU0FxgHLCXrFH8gIu5Lm75Z0hnp84yIqKtp3A78h6TVwKtkQYq0z3uA5cBu4EsR0SJNWmZmVozfEW5mZvvxO8LNzKxFOGiYmVlhDhpmZlaYg4aZmRXmoGFmZoU5aJiZWWEOGmZmVpiDhpmZFeagYWZmhTlomJlZYQ4aZmZWmIOGmZkV5qBhZmaFOWiYmVlhDhpmZlaYg4aZmRXmoGFmZoU5aJiZWWEOGmZmVpiDhpmZFVYoaEgaL2mVpNWSpjSQ5xJJyyUtk3RXLv3GlLZC0i2SlNInSloqaYmkByT1TOlzJC1O0zpJi1N6P0lv5Jb9W/mHb2ZmTVHVWAZJnYCZwHlALbBQ0ryIWJ7LMxCYCoyKiG2Sjk3pI4FRwNCU9QlgjKQngJuBIRHxR0k3AtcA0yPis7nt/gDYnivOmogY1vzDNTOzchSpaZwNrI6ItRHxNnA3cFFJniuAmRGxDSAiXknpAXQDugBdgc7AZkBpOjTVPI4Afp/fYEq/BPjPZhyXmZm1giJBozfwcm6+NqXlDQIGSXpS0gJJ4wEi4mngEWBTmuZHxIqI2AVcDSwlCxZDgNtLtjka2BwRL+bS+kt6TtJvJI2ur7CSJkuqkVSzZcuWAodnZmZFtVRHeBUwEBgLTARulXSUpJOBwUAfskAzTtJoSZ3JgsZw4ARgCVnzVt5E9q9lbAJOiojhwFeBuyQdUVqQiJgVEdURUd2rV68WOjwzM4MCfRrARuDE3HyflJZXCzyTahAvSXqBfUFkQUTsAJB0PzACeBMgItak9HuAvR3skqqATwNn1qVFxFvAW+nzIklryGo4NQWP1czMylSkprEQGCipv6QuwARgXkmee8kCBGkU1CBgLbCBrOO7KtUuxgAryILOEEl1VYHzUnqdPwdWRkRtXYKkXqlTHkkDyILS2iYcq5mZlanRmkZE7JZ0DTAf6ATcERHLJM0AaiJiXlp2vqTlwB7g+ojYKmkuMI6s7yKAByLiPgBJ3wIek7QLWA9cltvtBN7dAX4OMCPlfwe4KiJebe6Bm5lZ0ykiKl2GVlNdXR01NW69MjNrCkmLIqK6vmW+I9zMzApz0DAzs8IcNMzMrDAHDTMzK8xBw8zMCnPQMDOzwhw0zMysMAcNMzMrzEHDzMwKc9AwM7PCHDTMzKwwBw0zMyvMQcPMzApz0DAzs8IcNMzMrDAHDTMzK8xBw8zMCnPQMDOzwhw0zMysMAcNMzMrrFDQkDRe0ipJqyVNaSDPJZKWS1om6a5c+o0pbYWkWyQppU+UtFTSEkkPSOqZ0qdL2ihpcZo+ntvW1FSGVZI+Vt6hm5lZU1U1lkFSJ2AmcB5QCyyUNC8ilufyDASmAqMiYpukY1P6SGAUMDRlfQIYI+kJ4GZgSET8UdKNwDXA9JTvnyPi+yXlGAJMAE4DTgAekjQoIvY079DNzKypitQ0zgZWR8TaiHgbuBu4qCTPFcDMiNgGEBGvpPQAugFdgK5AZ2AzoDQdmmoeRwC/b6QcFwF3R8RbEfESsDqVzczM2kiRoNEbeDk3X5vS8gYBgyQ9KWmBpPEAEfE08AiwKU3zI2JFROwCrgaWkgWLIcDtue1dk5qt7pDUownlQNJkSTWSarZs2VLg8MzMrKiW6givAgYCY4GJwK2SjpJ0MjAY6EN2gR8nabSkzmRBYzhZU9MSsuYtgH8FPgAMIws0P2hKQSJiVkRUR0R1r169yj4wMzPbp9E+DWAjcGJuvk9Ky6sFnkk1iJckvcC+ILIgInYASLofGAG8CRARa1L6PcCUlLa5bqOSbgV+0YRymJlZKypS01gIDJTUX1IXss7oeSV57iULEKRRUIOAtcAGso7vqlS7GAOsILvYD5FUVxU4L6Uj6fjcdj8F/C59ngdMkNRVUn+yoPTbJhyrmZmVqdGaRkTslnQNMB/oBNwREcskzQBqImJeWna+pOXAHuD6iNgqaS4wjqzvIoAHIuI+AEnfAh6TtAtYD1yWdnmjpGEp/zrgylSOZalGshzYDXzJI6fMzNqWIqLSZWg11dXVUVNTU+limJl1KJIWRUR1fct8R7iZmRXmoGFmZoU5aJiZWWEOGmZmVpiDhpmZFeagYWZmhTlomJlZYQ4aZmZWmIOGmZkV5qBhZmaFOWiYmVlhDhpmZlaYg4aZmRXmoGFmZoU5aJiZWWEOGmZmVpiDhpmZFeagYWZmhTlomJlZYQ4aZmZWWKGgIWm8pFWSVkua0kCeSyQtl7RM0l259BtT2gpJt0hSSp8oaamkJZIekNQzpX9P0sqU/j+Sjkrp/SS9IWlxmv6t/MM3M7OmaDRoSOoEzAQuAIYAEyUNKckzEJgKjIqI04CvpPSRwChgKHA6cBYwRlIVcDPw0YgYCiwBrkmbexA4PaW/kLZbZ01EDEvTVc08ZjMza6YiNY2zgdURsTYi3gbuBi4qyXMFMDMitgFExCspPYBuQBegK9AZ2AwoTYemmscRwO/Tur+KiN1p/QVAn2Yem5mZtbAiQaM38HJuvjal5Q0CBkl6UtICSeMBIuJp4BFgU5rmR8SKiNgFXA0sJQsWQ4Db69n35cD9ufn+kp6T9BtJo+srrKTJkmok1WzZsqXA4ZmZWVEt1RFeBQwExgITgVslHSXpZGAwWW2hNzBO0mhJncmCxnDgBLLmqXwzFJKmAbuB2SlpE3BSRAwHvgrcJemI0oJExKyIqI6I6l69erXQ4ZmZGWQX+8ZsBE7MzfdJaXm1wDOpBvGSpBfYF0QWRMQOAEn3AyOANwEiYk1KvwfY28Eu6TLgk8C5EREp71vAW+nzIklryGo4NcUP18zMylGkprEQGCipv6QuwARgXkmee8kCBGkU1CBgLbCB1PGdahdjgBVkQWeIpLqqwHkpndS0dQNwYUTsrNuBpF6pUx5JA8iC0tomH7GZmTVbozWNiNgt6RpgPtAJuCMilkmaAdRExLy07HxJy4E9wPURsVXSXGAcWd9FAA9ExH0Akr4FPCZpF7AeuCzt8l/IOs0fTKNzF6SRUucAM1L+d4CrIuLVFjkLZmZWiFLrz0Gpuro6amrcemVm1hSSFkVEdX3LfEe4mZkV5qBhZmaFOWiYmVlhDhpmZlaYg4aZmRXmoGFmZoU5aJiZWWEOGmZmVpiDhpmZFeagYWZmhTlomJlZYQ4aZmZWmIOGmZkV5qBhZmaFOWiYmVlhDhpmZlaYg4aZmRXmoGFmZoU5aJiZWWEOGmZmVlihoCFpvKRVklZLmtJAnkskLZe0TNJdufQbU9oKSbdIUkqfKGmppCWSHpDUM6UfLelBSS+mnz1SutL6q9M6Hyr/8M3MrCkaDRqSOgEzgQuAIcBESUNK8gwEpgKjIuI04CspfSQwChgKnA6cBYyRVAXcDHw0IoYCS4Br0uamAL+OiIHAr9M8af8D0zQZ+NdmHrOZmTVTkZrG2cDqiFgbEW8DdwMXleS5ApgZEdsAIuKVlB5AN6AL0BXoDGwGlKZDU83jCOD3aZ2LgDvT5zuBi3PpP43MAuAoScc35WDNzKw8RYJGb+Dl3HxtSssbBAyS9KSkBZLGA0TE08AjwKY0zY+IFRGxC7gaWEoWLIYAt6dtHRcRm9LnPwDHNaEcSJosqUZSzZYtWwocnpmZFdVSHeFVZM1GY4GJwK2SjpJ0MjAY6EN2gR8nabSkzmRBYzhwAlnz1NTSjUZEkNVWCouIWRFRHRHVvXr1KuOQzMysVFWBPBuBE3PzfVJaXi3wTKpBvCTpBfYFkQURsQNA0v3ACOBNgIhYk9LvYV/fxWZJx0fEptT8VNfUVaQcZmbWiorUNBYCAyX1l9QFmADMK8lzL1mAII2CGgSsBTaQOr5T7WIMsILsYj9EUl1V4LyUTtr2penzpcD/5tI/l0ZRfQTYnmvGMjOzNtBoTSMidku6BpgPdALuiIhlkmYANRExLy07X9JyYA9wfURslTQXGEfWdxHAAxFxH4CkbwGPSdoFrAcuS7v8DnCPpC+k9EtS+i+BjwOrgZ3A58s+ejMzaxJl3QYHp+rq6qipqal0MczMOhRJiyKiur5lviPczMwKc9AwM7PCHDTMzKwwBw0zMyvMQcPMzApz0DAzs8IcNMzMrDAHDTMzK8xBoxXMng39+sEhh2Q/Z8+udInMzFpGkQcWWhPMng2TJ8POndn8+vXZPMCkSZUrl5lZS3BNo4VNm7YvYNTZuTNLNzPr6Bw0WtiGDU1LNzPrSBw0WthJJzUt3cysI3HQaGHf/jZ0775/WvfuWbqZWUfnoNHCJk2CWbOgb1+Qsp+zZrkT3MwODh491QomTXKQMLODk2saZmZWmIOGmZkV5qBhZmaFOWiYmVlhhYKGpPGSVklaLWlKA3kukbRc0jJJd+XSb0xpKyTdoszhkhbnpj9Kuinl/+dc+guS/pTb1p7csnnlHryZmTVNo6OnJHUCZgLnAbXAQknzImJ5Ls9AYCowKiK2STo2pY8ERgFDU9YngDER8SgwLLf+IuC/ASLiulz63wHDc8V5IyKGYWZmFVGkpnE2sDoi1kbE28DdwEUlea4AZkbENoCIeCWlB9AN6AJ0BToDm/MrShoEHAs8Xs++JwL/WexQDh5+Sq6ZtVdFgkZv4OXcfG1KyxsEDJL0pKQFksYDRMTTwCPApjTNj4gVJetOAOZEROQTJfUF+gMP55K7SapJ+7i4vsJKmpzy1GzZsqXA4bUvdU/JXb8eIvY9JdeBw8zag5bqCK8CBgJjyWoHt0o6StLJwGCgD1mgGSdpdMm6E6i/NjEBmBsRe3JpfSOiGvgr4CZJHyhdKSJmRUR1RFT36tWr3ONqc35Krpm1Z0WCxkbgxNx8n5SWVwvMi4hdEfES8AJZEPkUsCAidkTEDuB+YETdSpLOAKoiYlE9+31XMImIjennWuBR9u/vOCj4Kblm1p4VCRoLgYGS+kvqQnYxLx25dC9ZLQNJPcmaq9YCG4AxkqokdQbGAPnmqXr7LCSdCvQAns6l9ZDUNbePUcDy0nU7Oj8l18zas0aDRkTsBq4B5pNd8O+JiGWSZki6MGWbD2yVtJysD+P6iNgKzAXWAEuB54HnI+K+3OYvoeGmqbtL+jkGAzWSnk/7+E5+BNfBwk/JNbP2TCX9zweV6urqqKmpqXQxmmz27KwPY8OGrIbx7W/7AYhm1nYkLUr9x+/iO8LboUmTYN06eOed7GdTA4aH7JpZa/Gj0Q8ydUN260Zg1Q3ZBddWzKx8rmkcZDxk18xak4PGQcZDds2sNTloHGQ8ZNfMWpODxkHGQ3bNrDU5aBxkJk2CWbOgb1+Qsp+zZjWtE9yjr8ysIQ4aB6Fyhuy2xAMTHXTMDl4OGrafckdf+Sm9HT9odvTyW+ty0LD9lDv6qj0M+a3kRa+jB82OXn5rfX6MiO2nX7/sQlGqb9+sqasxhxySXWxKSVlzWWsrvbkRsoEATe3Xaa5yz1+ldfTyW8vwY0SssHJHX7XEkN9yagqVrul09PtkOnr5rfU5aNh+yh19VW7QKbd5pNIXvY5+n0xHL7+1PgcNe5dyRl+VG3TKrSlU+qLXEvfJVLJPpj3c5+OO+HYuIg7a6cwzzwzrWKSIrI6x/yQVWz0mzNIAAAiiSURBVP9nP4vo3n3/dbt3z9Lbys9+FtG3b1bmvn2btu+OXv6W2Helj98igJpo4Lpa8Qt7a04OGh1P3771B42+fYtvo9yLXiUvmi1x/JVWzvlrD8dfyd9/e+GgYR1Gpf/TrPT+y61pVVq556/Sx1/p339LaImg56BhHcp7+T/9Su+/XOWWv9LHX+n9l6ulgt6BgoY7wq3dKffNheWo9Oirjt6RXu75q3RHfKV//+VqiyHnDhpmOZUefVXu6LNK39Fd7vlriQdulqPS9xmVq02CXkNVkPwEjAdWAauBKQ3kuQRYDiwD7sql35jSVgC3AAIOBxbnpj8CN6X8lwFbcsu+mNvWpcCLabq0sXK7ecqaqqO3aVe6eaWjn79yy98Sx98eBhJQTp8G0AlYAwwAugDPA0NK8gwEngN6pPlj08+RwJNpG52Ap4Gx9exjEXBO7Asa/1JPnqOBtelnj/S5x4HK7qBhzdGRR89UuiM5ovLnr5Kj58q9aLeHoBVRftAYAczPzU8FppbkuTFfIyhZdxHwPqA7UAMMLskzCHiZfc/BaihoTAR+nJv/MTDxQGV30LD3mkrXNCqt0jWdcoN2exhyHnHgoFGkT6N3uqjXqU1peYOAQZKelLRA0niAiHgaeATYlKb5EbGiZN0JwJxU0Dp/IWmJpLmSTmxCOZA0WVKNpJotW7YUODyzg0elO5IrrdLPHiu3T6Ql+iRaeyBJS3WEV5E1UY0lqxHcKukoSScDg4E+ZBf4cZJGl6w7AfjP3Px9QL+IGAo8CNzZlIJExKyIqI6I6l69ejXrYMw6qkp3JFdapUc/tYcHfra2IkFjI3Bibr5PSsurBeZFxK6IeAl4gSyIfApYEBE7ImIHcD9ZkxUAks4AqiJiUV1aRGyNiLfS7G3AmU0oh9l7XiWHLFdapS+6lX7gZ1soEjQWAgMl9ZfUhaxmMK8kz71ktQwk9SRrrloLbADGSKqS1BkYQzaKqs5E9q9lIOn43OyFufzzgfMl9ZDUAzg/pZmZAe3jolvJB362harGMkTEbknXkF2gOwF3RMQySTPIOkvmse+CvhzYA1wfEVslzQXGAUuBAB6IiPtym78E+HjJLq+VdCGwG3iVrGOciHhV0j+RBTGAGRHxarOO2swOSnUX12nTsiapk07KAkZ7uug2ZtKk9l1ev7nPzMz24zf3mZlZi3DQMDOzwhw0zMysMAcNMzMrzEHDzMwKO6hHT0naAqwvYxM9yZ7A2165fOVx+crj8pWnPZevb0TU+0iNgzpolEtSTUPDztoDl688Ll95XL7ytPfyNcTNU2ZmVpiDhpmZFeagcWCzKl2ARrh85XH5yuPylae9l69e7tMwM7PCXNMwM7PCHDTMzKyw93zQkDRe0ipJqyVNqWd5V0lz0vJnJPVrw7KdKOkRScslLZP05XryjJW0XdLiNP1jW5UvV4Z1kpam/b/rscLK3JLO4RJJH2rDsp2SOzeLJb0m6Ssledr0HEq6Q9Irkn6XSzta0oOSXkw/ezSw7qUpz4uSLm3D8n1P0sr0+/sfSUc1sO4BvwutWL7pkjbmfoelr1yoy3fAv/dWLN+cXNnWSVrcwLqtfv7K1tDLw98LE9n7QdYAA4AuwPPAkJI8fwv8W/pc9z7ztirf8cCH0ufDyd6IWFq+scAvKnwe1wE9D7D842RvbRTwEeCZCv6+/0B241LFziFwDvAh4He5tBuBKenzFOC79ax3NNnLzY4GeqTPPdqofOeTvWUT4Lv1la/Id6EVyzcd+IcCv/8D/r23VvlKlv8A+MdKnb9yp/d6TeNsYHVErI2It4G7gYtK8lzEvveUzwXOlaS2KFxEbIqIZ9Pn18neYti7Lfbdwi4CfhqZBcBRJW9obCvnAmsiopynBJQtIh4je8FYXv57didwcT2rfgx4MCJejYhtwIPA+LYoX0T8KiJ2p9kFZK9brogGzl8RRf7ey3ag8qVrxyWUvLG0I3mvB43ewMu5+VrefVHemyf90WwHjmmT0uWkZrHhwDP1LB4h6XlJ90s6rU0LlgngV5IWSZpcz/Ii57ktTKDhP9ZKn8PjImJT+vwH4Lh68rSX83g5Wc2xPo19F1rTNan57I4Gmvfaw/kbDWyOiBcbWF7J81fIez1odAiSDgN+DnwlIl4rWfwsWXPLGcD/I3tfe1v7s4j4EHAB8CVJ51SgDAek7P32FwL/Vc/i9nAO94qsnaJdjoWXNI3sVcyzG8hSqe/CvwIfAIYBm8iagNqjiRy4ltHu/5be60FjI3Bibr5PSqs3j6Qq4Ehga5uULttnZ7KAMTsi/rt0eUS8FhE70udfAp0l9Wyr8qX9bkw/XwH+h6wZIK/IeW5tFwDPRsTm0gXt4RwCm+ua7NLPV+rJU9HzKOky4JPApBTY3qXAd6FVRMTmiNgTEe8Atzaw30qfvyrg08CchvJU6vw1xXs9aCwEBkrqn/4TnQDMK8kzD6gbpfIZ4OGG/mBaWmr/vB1YERE/bCDP++v6WCSdTfY7bcugdqikw+s+k3WY/q4k2zzgc2kU1UeA7bmmmLbS4H94lT6HSf57dinwv/XkmQ+cL6lHan45P6W1OknjgRuACyNiZwN5inwXWqt8+T6yTzWw3yJ/763pz4GVEVFb38JKnr8mqXRPfKUnspE9L5CNqpiW0maQ/XEAdCNr0lgN/BYY0IZl+zOyZoolwOI0fRy4Crgq5bkGWEY2EmQBMLKNz9+AtO/nUznqzmG+jAJmpnO8FKhu4zIeShYEjsylVewckgWvTcAusnb1L5D1k/0aeBF4CDg65a0Gbsute3n6Lq4GPt+G5VtN1h9Q9z2sG1F4AvDLA30X2qh8/5G+W0vIAsHxpeVL8+/6e2+L8qX0n9R953J52/z8lTv5MSJmZlbYe715yszMmsBBw8zMCnPQMDOzwhw0zMysMAcNMzMrzEHDzMwKc9AwM7PC/j+FBj4FPhE/2QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JvvnYHBCfj04"
      },
      "source": [
        "The second model will be based on LSTMs.\n",
        "\n",
        "- Use the following layers to build your network\n",
        "  - An embedding layer with top_words = 5000, an embedding vector length of 32 and a max reviex length of 500\n",
        "  - A Dropout layer with dropout ratio of 0.2\n",
        "  - An LSTM layer with 100 units\n",
        "  - A Dropout layer with dropout ratio of 0.2\n",
        "  - An output fully-connected layer with one unit and a Sigmoid activation function.\n",
        "- Compile the generator using binary crossentropy loss and the Adam optimizer \n",
        "- Train on the data with 3 epochs\n",
        "- Compute the accuracy with model.evaluate\n",
        "- Plot the history of the loss versus the epoch number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aNYrzga3fjin",
        "outputId": "085c24f6-7fe3-4959-8e74-f36bf0cf0293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "#TODO\n",
        "model = Sequential()\n",
        "model.add(Embedding(5000, 32, input_length=500))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train,epochs=3,)\n",
        "results = model.evaluate(X_test, y_test, batch_size=128)\n",
        "\n",
        "print('test loss, test acc:', results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "782/782 [==============================] - 20s 25ms/step - loss: 0.4669 - accuracy: 0.7721\n",
            "Epoch 2/3\n",
            "782/782 [==============================] - 20s 25ms/step - loss: 0.3036 - accuracy: 0.8764\n",
            "Epoch 3/3\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.3242 - accuracy: 0.8626\n",
            "196/196 [==============================] - 3s 13ms/step - loss: 0.3892 - accuracy: 0.8352\n",
            "test loss, test acc: [0.3892463147640228, 0.8352400064468384]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68ORUhSfRjza",
        "colab_type": "code",
        "outputId": "fe1fbb47-6be6-4163-9228-c18a99fa324a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "train_loss = history.history['loss']\n",
        "\n",
        "plt.plot(train_loss)\n",
        "plt.title('Training Loss x Epochs')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMcYJlGmSE4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}